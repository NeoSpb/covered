\section{Section 7.  Test and Checkout Procedure}\label{page_testing}
\begin{Desc}
\item[Section 7.1. Testing Methodology]\end{Desc}
\begin{Desc}
\item[]Testing the Covered tool for general \char`\"{}goodness\char`\"{}, which is required for release, is accomplished with a suite of C and Verilog diagnostics. These diagnostics are located in the \char`\"{}diag\char`\"{} directory within the main Covered directory. These suite of tests are run in a regression manner; that is, each diagnostic is self-checking and run in serial order. The C regression is run from the \char`\"{}c\char`\"{} directory while the Verilog diagnostic regression is run from the \char`\"{}regress\char`\"{} directory. The C regression test is used to test out specific functions within the code that might otherwise not be adequately tested/testable in the full system. The Verilog diagnostic suite is used to verify that Covered works correctly as a whole. The following subsections describe the testing methodology used by both test suites.\end{Desc}
\begin{Desc}
\item[Section 7.1.1. C Testing Methodology]\end{Desc}
\begin{Desc}
\item[]The C regression test suite consists of single file C code that includes the necessary header files from the \char`\"{}src\char`\"{} directory and specifies the necessary source files from the \char`\"{}src\char`\"{} directory in the linking phase. Each C diagnostic must contain a \char`\"{}main()\char`\"{} routine, and it must print the keyword \char`\"{}PASSED\char`\"{} if the diagnostic was considered successful or print the keyword \char`\"{}FAILED\char`\"{} if the diagnostic was not considered successful. This output must be sent to standard output as well as the regression output file \char`\"{}regress.log\char`\"{}. After all diagnostics are run, the output file is grep'ed for the keyword \char`\"{}PASSED\char`\"{}. The number of diagnostics finishing the PASS message are compared against the total number of diagnostics. The results of which are output to standard output.\end{Desc}
\begin{Desc}
\item[Section 7.1.2. Verilog Testing Methodology]\end{Desc}
\begin{Desc}
\item[]The Verilog regression suite consists of four directories: regress, verilog, cdd, and rpt. The regress directory is the directory where all Verilog regressions are run from. In this directory is the main Makefile and supporting $\ast$.cfg files. Each $\ast$.cfg file is named after its corresponding Verilog diagnostic file and in it contains all of the options to be passed to Covered on the score command-line. The $\ast$.cfg file is passed to Covered using the score command's '-f' option.\end{Desc}
\begin{Desc}
\item[]The verilog directory contains all of the Verilog diagnostic files, Verilog include files and Verilog library files necessary to run regression. Additionally, two extra files \char`\"{}Makefile\char`\"{} (Makefile in charge of running the Verilog diagnostics and verifying their output) and \char`\"{}check\_\-test\char`\"{} (Perl script used to perform verification of test passing) exist to handle diagnostic running and output checking. All output from a regression run is also placed into this directory.\end{Desc}
\begin{Desc}
\item[]The cdd directory contains a generated/scored CDD file for each Verilog diagnostic. The CDD files in this directory are CDD files determined to be good by the test writer. When a diagnostic is run in which the output is deemed to be good, the $\ast$.cdd file from the diagnostic run is copied to this directory. All CDD files in this directory are used to compare the final CDD output from a diagnostic run to determine if the generated CDD file is correct. Compares are performed via the \char`\"{}diff\char`\"{} Unix command.\end{Desc}
\begin{Desc}
\item[]The rpt directory contains a generated module ($\ast$.rpt\-M) and instance ($\ast$.rpt\-I) report for each Verilog diagnostic. Like the cdd directory, the rpt directory contains generated reports that were deemed to be correct by the diagnostic writer. When diagnostic is run in which the output is deemed to be good, the $\ast$.rpt\-I/$\ast$.rpt\-M files from the diagnostic run are copied to this directory. All report files in this directory are used to compare the final report outputs from a diagnostic run to determine if the generated report files are correct. Compares are performed via the \char`\"{}diff\char`\"{} Unix command.\end{Desc}
\begin{Desc}
\item[]When a Verilog regression is run (from the \char`\"{}regress\char`\"{} directory), all diagnostics are run through the score and report commands (only a handful of diagnostics are run through the merge command also) in the \char`\"{}verilog\char`\"{} directory. The generated CDD, module report and instance report files are placed into the \char`\"{}verilog\char`\"{} directory. When these output files are generated for a diagnostic, the check\_\-test script is run for that diagnostic to compare the new outputs with the known good outputs. If an output file is not found to differ from the golden version, the newly generated output file is removed from the verilog directory. If an output file is found to differ from the golden version, the output file is not removed from the verilog directory (this makes it easier to identify which diagnostics failed after regression). If all output files match, the keyword \char`\"{}PASSED\char`\"{} is sent to standard output and the passed counter in the \char`\"{}regress.output\char`\"{} file is incremented by one. If at least one output file does not match, the keyword \char`\"{}FAILED\char`\"{} is sent to standard output and the failed counter in the \char`\"{}regress.output\char`\"{} is incremented by one. If a diagnostic is considered a failure, regression continues to run until all diagnostics have been tested. After all diagnostics have been run, the contents of the \char`\"{}regress.output\char`\"{} file is output to standard output to indicate the number of passing and failing diangostics.\end{Desc}




\begin{Desc}
\item[Section 7.2. Testing Directories]\end{Desc}
\begin{Desc}
\item[]The reason for having two directories for regression testing relies on the feature under test. Verilog diagnostics are condensed DUTs which only contain the required code for testing a particular syntax of the Verilog language to verify that Covered is able to correctly parse the code and generate the appropriate coverage results for that feature. All Verilog diagnostics are accompanied by a text file that is used for comparison purposes. The Makefile, after simulating the Verilog file, creating the dumpfile, generating the CDD and generating a verbose report based on the CDD will compare the generated report to the text file by performing a UNIX \char`\"{}diff\char`\"{} command. If the results of the \char`\"{}diff\char`\"{} are no differences between the two files, the Makefile will assume that the diagnostic has successfully passed and output the keyword \char`\"{}PASSED\char`\"{} to the output result file. If the results of \char`\"{}diff\char`\"{} show that there are differences between the two files, the Makefile will assume failure and output the keyword \char`\"{}FAILED\char`\"{} to the output result file.\end{Desc}
\begin{Desc}
\item[]C diagnostics exist to test certain functions of Covered rather than the entire tool itself. Many times it is impossible/impractical to create Verilog diagnostics that exercise certain functions within Covered to completion. In these cases, it is often easier to write more specialized tests that can more quickly manipulate inputs to functions and verify that all output values are correct. Examples of C diagnostics that currently exist in the C regression directory include tests of bitwise operators, mathematical functions, etc.\end{Desc}
\begin{Desc}
\item[]It is suggested that if functions can be adequately tested at a system level, that it be done so using Verilog diagnostics as these will get the most testing out of the entire tool. However, if functions are best tested in seclusion, it is suggested that the C testing environment be used.\end{Desc}




\begin{Desc}
\item[Section 7.3. Verilog Testing Procedure]\end{Desc}




\begin{Desc}
\item[Go To Section...]\begin{itemize}
\item {\bf Section 1.  Introduction}{\rm (p.\,\pageref{page_intro})}\item {\bf Section 2.  Project Plan}{\rm (p.\,\pageref{page_project_plan})}\item {\bf Section 3.  Coding Style Guidelines}{\rm (p.\,\pageref{page_code_style})}\item {\bf Section 4.  Development Tools}{\rm (p.\,\pageref{page_tools})}\item {\bf Section 5.  Project \char`\"{}Big Picture\char`\"{}}{\rm (p.\,\pageref{page_big_picture})}\item {\bf Section 6.  Coverage Development Reference}{\rm (p.\,\pageref{page_code_details})}\item {\bf Section 8.  Debugging}{\rm (p.\,\pageref{page_debugging})}\item {\bf Section 9.  Odds and Ends Information}{\rm (p.\,\pageref{page_misc})} \end{itemize}
\end{Desc}
